---
title: "Lab Notebook: May 2018"
output: html_notebook
---
#Goals: May 21 - 25
1. ASM blog post
    + incorporate edits from group to fungi survey post

1. Coursera Stats: Inferential Stats wk 4

1. XML parse - next step is scale up to all AEM XML files, waiting until after ASM

1. ASM Transfer articles
    + ~~incorporate total rejection numbers into analysis~~
    + will probably need to do an analysis over time to see trend in published vs unpublished
    + ~~draft of Rmarkdown report for Pat~~
    + finalize Rmarkdown report & send to Melissa    + 
    
1. _Bacillus_ manuscript - Phil's edits? Submit?
    
1. Estimated IF
    + Waiting for WoS API update from Pat
    + Figure out WoS API

#2018/05/22

**- ASM transfer**

Worked on updating the Rmarkdown analysis & sent draft to Pat. Trying to install `tinytex` so that I can generate a PDF version of the report but the install keeps failing. I've tried installing TeX Studio & the complete MiTex, but neither seemed to work, so I tried to uninstall both of them. 

Pat said to do word doc then reformat. He gave suggestions, for changes. One major thing is to eliminate articles whose resubmissions were accepted to the same journal they were rejected from. Notes for Melissa - Parasitology is showing up in ASM submitted journal, and some journals don't have articles given reject & resubmit decisions associated with them. Need to cut the "moving journals" section, merge with mBio & eliminate transfer days analysis. Pat wants me to compare median ASM citations to non-rejected ASM citations, which I'll have to pull from Impact Vizor. Fiddle with Log2 transformed citation plots.

#2018/05/21

**- ASM transfer**

Worked on updating the Rmarkdown analysis. Need to fix the copy/editing language, rename the Journals so that they're capitalized correctly and figure out why cmr & mmbr aren't grouping correctly.

**- Bacillus manuscript**

Check in with Phil.

#Goals: May 14 - 18, 2018
1. ASM blog post - fungi purify heavy metals
    + ~~incorporate suggestions from Jesus~~
    + Microbial Myths post - postpone until after ASM

1. ~~Read Sarah's paper for journal club~~

1. ~~Coursera Stats: Inferential Stats wk 3~~

1. XML parse
    + ~~finish & test script to group manuscripts & join as column~~
    + ~~figure out how to run an r script from flux~~
    + check out parsing script with 100 more xml files on flux

1. ASM Transfer articles
    + ~~waiting for total numbers of rejections from Stacey~~
    + incorporate total rejection numbers into analysis
    + will probably need to do an analysis over time to see trend in published vs unpublished
    + finalize Rmarkdown report
    
1. _Bacillus_ manuscript
    + finish incorporating comments (recieved: Zack, Ashu, Steve, David)
    
1. Estimated IF
    + Waiting for WoS API update from Pat
    + Figure out WoS API

#2018/05/17

**- Lab meeting**

**- Journal Club paper**

**- ASM Transfer Analysis**

Pulled new data from Impact Vizor to analyze along with the rejected totals from Stacey. These data include ALL rejected submissions from 01/01/2013 to 04/30/2018. My prior analysis only looked at full text research articles, so I will have to start broad & then narrow the scope to research articles & short forms. Fortunately, the new data include citation data for mSystems & mSphere articles so I don't have to re-pull that data. Tried doing a ggplot looking at rejections over time, but had trouble with the layering. I'll need to somehow get all of the data in the same dataframe? Mostly spent time going over the previous analysis updating the variable names & incorporating the total rejected data. I'll have to complete that with the rest of the analysis & then update the memo. I'm not sure the best way to do that, either update what I have or just start from scratch.

#2018/05/16

**- ASM Blog post**

Work on Jesus' suggested edits & sent to Ray.

**- Coursera: Inferential Stats wk 3**

Comparing two paired means:

  + When two sets of observations are not independent, they are paired (e.g., reading & writing)
  
  + look at the **difference** in the two parameters
  
  + parameter of interest: average diff for population (mu diff), 
  
  + point estimate: average difference from sample (xbar diff)
  
  + Hypothesis for paired means: H0 == no difference, HA == is a difference 
  
Power:

  + The probability of correctly rejecting H0 == 1 - beta (the probability of a type 2 error - failing to reject H0)
  
  + H0: no difference, HA: there is a difference
  
  + work backwards from the desired power to determine the desired sample size
  
  + calculate required sample size for a desired level of power
  
  + calculate power for a range of sample sizes & choose the level of power
  
Comparing >2 means:

  + ANOVA - analysis of variance, with test statistic F
  
  + H0 - the mean outcome is the same across all categories (mu1 = mu2 = m3 ... = muk)
  
  + HA - at least one pair of means is different from each other (doesn't specify which are different)
  
  + T test compares means from **2** groups: are they so far apart that the observed difference cannot be reasonably attributed to sampling variability & test statistic is a ratio of std difference over std error
  
  + ANOVA - compare means from **more than two** groups: are they so far apart that the observed differences cannot all reasonably be attributed to sampling variability & test stat is a ratio of the variability between groups over the variability within groups. 
  
  + Large test stats lead to small p-values
  
  + F distribution is right skewed & always positive. A large F stat requires that the variability between groups is larger than the variability within groups.
  
  + Variability partitioning - between groups (based on grouping variablity) & other factors (within group variability)
  
  + SST (sum of squares total) measures Total variability in the response variable - more interested in how this value partitions 
  
  + SSG (sum of squares groups) measures variability Between groups - explained variablity (squared deviation of group means from overall mean, weighted sample size)
  
  + SSE (sum of squares error) measures the variability within groups - unexplained variability (unexplained by the group variable due to other reasons)
  
  + To get from SS to Mean Sq, weight by sample size using DF.
  
  + Mean sq, which measures average variability within & between groups, calculated as the total variability (SS) scaled by the associated degrees of freedom
  
  + F stat = ratio of the avg between group & within group variabilites (i.e., MSG/MSE)
  
  + p-value is the probaility of at least as large a ratio between the "between" & "within" group variabilities if in fact the means of all groups are equal - area under the F curve, with DFF & DFE 
  
  + to calculate `pf(F stat, DFg, DFe, lower.tail = FALSE)`
  
Conditions for ANOVA

  1. Indepdendence:
    i. within groups: sampled observations must be independent
    i. between groups: the groups must be independent of each other (non-paired) -- if not, repeated measures ANOVA
  1. Approximate normality: distributions should be nearly normal within each group
  1. Equal variance: groups should have roughly equal variability - can check with box plots
  
Multiple comparisons

  + ANOVA tells that means differ, but not which
  
  + multiple comparisions inflates the Type 1 error rate, correct with multiple comparisions significance level
  
Bootstrapping:

  + Using samples to make conclusions about a population when you can't sample the population further
  
  + Scheme:
  
    1. take a bootstrap sample - a random sample taken **with replacement** from the original sample, of the same size of the original sample
    
    1. calculate the bootstrap stat - a stat such as mean, median, proportion, etc computed on the bootstrap samples
    
    1. repeat steps 1 & 2 many times (generally >10,000) to create a bootstrap distribution - a distribution of bootstrap statistics
    
  + Estimate the confidence interval:
  
    1. percentile method
    
    1. standard error method
    
  + Limitations:
  
    + Not as rigid conditions as CLT based methods
    + if the bootstrap distribution is extermely skewed or sparse, the bootstrap interval might be unreliable
    + A representative sample is still required -- if the sample is biased, the estimates resulting from this sample will also be biased. 
    
  + Bootstrap vs sampling distributions:
  
    + Sampling dist created using sampling (with replacement) from the population
    + Bootstrap dist created using sampling (with replacement) from the sample
    + Both are distributions of sample statistics

**-ASM Transfer articles**

Recieved rejection data from Stacey

**- Update Strategic goals**

**- Flux**

Worked on trying to randomly select & move files again, to no avail.

#2018/05/15

**- ASM blog post**

Edit Monika's next post.

**- Flux**

I need to randomly select 100 XML files from the `gender_data/data/AEM` folder & copy them to `XML_parse/Files`, but that is apparently pretty tricky. At least I know I can write `Rscript path/to/code/` to run an R script.

**- Phone meeting with Melissa**

Talking about the monthly reports. Pulled from eJP, generally canned search queries. Screenshot, pasted & save as PDF vs. pulling from/reformatting excel sheet. Pat really likes the mBio report, which is more detailed. EJP has APIs, Melissa will write down the parameters for the current reports, then put me in communication with an eJP rep in order to work out the details of their APIs. I sent her a draft version of the rejection report. 

**- XML parse**

It took all FUCKING day, but I finally figured out how to link the current manuscript number to the grouped manuscript number using `mutate`, `map` and my own function.

#2018/05/14

**- XML parse**

Doing a conditional join using strings is more difficult than it sounds. Tried doing a mutate, but because it works as a vector vs. row by row basis, it doesn't work with any of the `stringr` functions & returns a vector instead of a single `group_number`. I'm probably going to have to do a join, but I think I'm running into the same problem with the `fuzzy_left_join`. I tried `sqldf` but that doesn't want to work at all. 

**- Bacillus manuscript**

Check in with Phil about preprint & submissions. Sent the revised draft to Phil.

**- Coursera Stats: Inferential Stats wk 3**

t-distribution:

  + when sigma is unknown (almost always), use the t-distribution to address the uncertaininty of the std error estimate, bell shaped but thicker tails than the normal (observations more likely to fall beyond 2 SD from the mean - more conservative)
  
  + always centered at 0, with one parameter: degrees of freedom (df), which determines the thickness of of tails
  
  + t statistic: T = (obs - null)/SE 
  P(|tdf=10|>2) = `pt(2, df = 10, lower.tail = FALSE) *2`
  
Inference for a mean:

  + estimate the mean: point estimate +/- margin of error == xbar+/- t*df(s/sqrt(n)) where df = n-1
  
  + finding the critical t score: `qt(lower tail %, df = )` - always use positive critical score
  
  + finding the p-value: `pt(t score, df, lower.tail = FALSE) *2` (pt gives the probability BELOW on the curve but we need the inverse so x2)
  
Comparing two independent means:

  + point estimate is the DIFFERENCE between the two means (Xbar1 - xbar2), do the same with the SE for each xbar
  
  + DF for the t statistic is the minimum n-1
  
  + Conditions for inference for comparing two independent means:
    1. Independence:
        + within groups: sampled observations must be independent (random sample, <10% of population)
        + between groups: the two groups must be independent of each other (non-paired)
    2. Sample size/skew: The more skew in the population distributions, the higher the sample size needed
    
  

#Goals: May 7 - 11, 2018
1. ASM blog post - fungi purify heavy metals
    + ~~send first draft of fungi survey to Jesus~~
    + ~~start outline for Ergot~~

1. ~~Read Marcy's paper for journal club~~

1. ~~Coursera Stats: Inferential Stats wk 2~~

1. XML parse
    + ~~using lab meeting ideas, write script to group reject & resubmit papers~~
    + figure out how to run an r script from flux
    + check out parsing script with more xml files on flux

1. ASM Transfer articles
    + waiting for total numbers of rejections from Stacey
    + incorporate total rejection numbers into analysis
    
1. _Bacillus_ manuscript
    + N/A - waiting for comments (recieved: Zack, Ashu, Steve, David)
    
1. Estimated IF
    + Figure out WoS API

1. ~~Meet w. Joel Baizra - Mon @ 1:30 (talk about publishing)~~

#2018/05/10

**- Lab meeting**

**- Altmetric workshop**

Altmetric explorer allows you to visualize the data for all things that are being tracked - NOT restricted to UM research!

Engagement: direct link back to research object

**- Flux rabbit hole**

Created a new github repository & cloned it to flux. Copied `gender_data` into it and added to `.gitignore`. Figured out that Powershell doesn't like Nano so I messed with text editors. Looks like Vim is my best option (cause Atom takes FOREVER to open). Have to figure out how to download new colorschemes for Vim. Still have to figure out how to run a script from within flux...

#2018/05/09

**- Bacillus Manuscript**

Incorporate David's edits.

**-ASM blog post**

Edit post for this week.

Finish writing fungi survey post, gathered images, & sent to Jesus.

Rough outline for the ergot post. 

#2018/05/08

**- ASM blog post**

Write fungi survey.

**Bacillus manuscript**

Incorporated suggested edits from Zack, Ashu, & Steve. 

**- Coursera Stats: Inferental stats wk 2**

Hypothesis testing:

  + null - H0 - often either a skeptical persepective or claim to be tested (==)
  
  + alternative- HA - represents an alternative claim under consideration and is often represented by a range of possibile paramater values. (<,>, !=)
  
  + The skeptic will not abandon the H0 unless the evidence in favor of the HA is so strong that she rejects H0 in favor of HA
  
  + hypothesis are always about POPULATION parameters (mu, sigma), never about sample statistics (xbar, St dev)
  
P-value - liklihood of observed or more extreme outcome given that H0 is true

  + test statistics (e.g. Z score) - used to test a hypothesis
  
  + used the test statistic to calculate the p-value, the probability of observing data at least as favorable to HA as our current data set, if the null hypothesis was true.
  
  + if p-value is low (lower than the significance level alpha, which is usually 5%), we say that it would be very unlikely to observe the data if the null hypothesis were true and **hence reject H0**
  
  + if p-value is high (> alpha), we that it is likely to observe the data even if the null hypothesis were true and hence **do not reject H0**
  
Two-sided tests:
  
  + Used to check for divergence in any direction, the definition of a p-value is the same, bu the calculation is slightly different since we have to consider "at least as extreme as the observed outcome" in both directions
  
  + use probability of both lower & upper tail (i.e., multiply probabilty of one tail x2)
  
**Hypothesis testing for a single mean:**

1. Set the hypothesis: 
    + H0: mu = null value
    + HA: mu < or > or != null value

2. Calculate the point estimate: xbar

3. Check conditions:
  i. Independence: Sampled observations must be independent (random sample/assignment & if sampling without replacement n<10% of population)
  i. Sample size/skew: n>= 30, larger if the population distribution is very skewed
  
4. Draw sampling distribution, shade p-value, calculate test statistic:
  i. Z = (xbar - mu)/SE
  i. SE = s/sqrt(n)
  
5. Make a decision, and interpret it in context of the research question.
  + if p-value < alpha, reject H0; the data provide convincing evidence for HA
  + if p-value >alpha, do not reject H0; the data do not provide convincing evidence for HA

Inference for other estimators:

  + An important assumption about point estimates is that they are **unbiased**, i.e., the sampling distribution of the estimate is centered at the true population parameter it estimates
    i. that is, an unbiased estimate doesn't naturally over or underestimate the parameter, it provides a "good" estimate
    i. the sample mean is an example of an unbiased point estimate as well as others (xbar1 - xbar2, sample proportion (p-hat), p-hat1 - p-hat2)
    
  + confidence intervals for nearly normal point estimates: point estimate +/- z* x SE
  
  + Z = (point estimate - null value)/SE
  
Decision Errors:

  + Type 1 errors: rejecting H0 when it is actually true, the probability of doing so is alpha
  
  + Type 2 errors: failing to reject H0 when HA is true (i.e., you should have rejected H0), the probability of doing so is beta
  
  + Balancing error rates: Type 1 & 2 errors are inversely proportional, tend to want to minimze the type 1 error
    
  + type 1 error rate:
    i. We reject H0 when the p-value < 0.05 (alpha == 0.05)
    i. this means that for the cases where H0 is actually true, we do not want to incorrectly reject it more than 5% of those times
    i. in other words, when using a 5% significane level, there is about 5% chance of making a type 1 error if the null hypothesis is true P(Type 1 error | H0 true) == alpha
    
  + Power of a test is the probability of correctly rejecting H0 & the probability of doing so is 1-beta
  
  + Goal is to keep both alpha & beta low
  
  + Type 2 error rate:
    i. if HA is actually true, what is the chance that we make a type 2 error?
    i. the answer isn't obvious
    i. if the true population average is very close to the null value, it will be difficult to detect a difference (& reject H0)
    i. if the true population average is very different from the null value it will be easier to detect a difference
    i. beta then depends on effect size (gamma), difference between point estimate & null value
    
Significance vs Confidence Levels:

  + complements of each other (e.g., 5% vs 95%) - depends on if test is one-sided vs two-sided
  
  + if two-sided, 95% interval with 2.5% on either side: alpha is equivalent to CL = 1 - alpha
  
  + if one-sided, then all 5% must be on one side, the equivalent confidence level is 90% b/c of the 5% at the other end of the tail: alpha is equvalent to CL = 1 - (2x alpha)
  
  + if H0 is rejected, a confidence interval that agrees with the result of hypothesis test should NOT include the null value
  
  + if H0 is failed to be rejected, a confidence interval that agrees with the result of the hypothesis test SHOULD include the null value
  
Statistical vs Practical Significance:

  + Real differences between the point estimate & null value are easier to detect with larger samples
  
  + However, very large samples will result in statistical significance even for tiny differences between the sample mean & the null value (effect size), even when the difference is not practically significant.

**- lab maintenance**

ADA & Shipping biologics training courses.

Applied for free github private repositories.

#2018/05/07

**- meeting with Dr. Baizra**

Notes: there are a series of challenges involved for African researchers publishing in ASM & other society/Western journals. 

1. Finances: anything more than $100 (30-50 is preferable) is too expensive for a researcher in a developing country to afford unless they are working with collaborators who have a grant to pay the funds. Universities (typically private) incentivize their researchers to publish, but aren't generally willing to help cover the cost of publishing, typically preferring that researcher submit to journals with publishing waivers. Government universities are sometimes willing/able to subsized, but not more than the 100 mentioned before. BMC is one example that offers waivers, but the review process is very long and drawn out, making it very challenging for students. They also have a high rejection rate.

An example journal is Science Domain, which Bazira has published with before. They have a $50 publication fee with very responsive peer reviewers (once recieved 7 reviewers for a single article!) He recommends them to students b/c they don't reject papers on the basis of novelty.

1. Subscription fees: If an article is blocked behind a paywall, then other African researcher that find it relevant cannot read it and thus cannot cite it. OA access papers are cited frequently though (Joel suggested that he gets at least one citation notification per week).

1. Citations: African researchers can't cite ASM journal articles because they don't find relevant articles to cite, e.g. research from others in their field - creates a bad feedback loop for ASM journals

1. Grammar/Reviewers: Some reviewers comment that the article needs a native English-speaker to review (often stated in a poorly constructed sentence). Some journals will recommend editing services, but again, that costs money. Bazira will often re-read the manuscript to identify issues but without more specific comments/examples, it is difficult for them to figure out what the reviewer might have had a problem with. An aside: truthfully, the researchers from some African countries may speak/write better English than we do. In Uganda, for example, there isn't a national language, primarily different dialects but their offical language is English, so children recieve their education in English from kindergarten. 

Takeaway: There is a market in Africa for a non-predatory research journal that will publish quality OA articles for a small fee and doesn't discriminate based on novelety. There might also be market for copy-editors -- charge U.S., but do African for small fees or pro-bono. Also a possibility to apply for a grant from World Bank or WHO to launch such a journal. Bazira notes that journals should be like bicycles, one for every stage of a career, not different quality, just different attention and goals. 

**- postdoc lunch with Dr. Baizra**

**- Journal club paper**

**- XML parse**

Experimented with the joins to link `related.manu` and `manuscript.number` columns to create a group id for each manuscript. Nick's idea almost works... Originally, I seemed to lose specificity behind the join.  However, this is getting me close, though I have to [save each iteration](https://stackoverflow.com/questions/36218643/save-all-iteration-result-of-repeat-loop-to-workspace-in-r) inorder to bind together at the end. If I do it this way, then I can see when matches stop b/c there's an "NA" at the end of the string ([`all(is.na())`](https://stackoverflow.com/questions/9417391/how-to-check-if-entire-vector-has-no-values-other-than-na-or-nan-in-r) should be helpful for that). I should be able to work this into a `repeat` function, one issue is the changing column names as they are added. 
```{r, eval = FALSE}
join_one <- input_df %>% filter(is.na(related.manu)) %>% 
  select(manuscript.number) %>% 
  left_join(., input_df, by = c("manuscript.number"="related.manu"))

join_two <- join_one %>%  
  filter(manuscript.number.y != "NA") %>% 
  left_join(., input_df, by = c("manuscript.number.y"="related.manu")) 

one_unite <- join_one %>% filter(is.na(manuscript.number.y)) %>% unite(sep = ";")

two_unite <- join_two %>% filter(is.na(manuscript.number.y.y)) %>% unite(sep = ";")

complete <- rbind(one_unite, two_unite)
```

Might be worth checking into the [`sqldf`](https://cran.r-project.org/web/packages/sqldf/sqldf.pdf) package based on the [SQL explainer blog post](https://academy.vertabelo.com/blog/illustrated-guide-sql-self-join/). 

Was able to logon to the `schloss-lab` flux directory.

**- Prep for meeting w. Joel Bazira**

Several publications (40+), I found one in JCM & several in... odd journals. i.e., ones that to my eyes would look predatory. Starting questions:
1. how do you choose where to submit, what journals are your "Safe" and "reach" journals?
1. what has your experience with publishing been like? ASM? 
1. would you like to see anything change in publishing?
    
#Goals: Apr 30 - May 4, 2018
1. ASM blog post - fungi purify heavy metals
    + finish first draft

1. ~~Plan for Code Review~~

1. ~~Coursera Stats: Inferential Stats wk 1~~

1. XML parse
    + ~~figure out how to link reject & resubmit papers to their accepted/published version --> while loop?~~
    + ~~figure out genderize.io~~
    + ~~figure out flux login~~

1. ASM Transfer articles
    + waiting for total numbers of rejections from Stacey
    
1. _Bacillus_ manuscript
    + ~~Edit text as needed~~
    
1. Estimated IF
    + Figure out WoS API
    
1. ~~ASM Staff Meeting - Monday @ 2pm~~

#2018/05/04

**- ASM rejections**

Pinged Stacey on this again.

**- Matt Foley's Thesis Defense**

**- Bacillus Manuscript**

Formatted Zacks figure & sent another draft out to authors. Just waiting for comments!

#2018/05/03

**- Lab meeting**

Pat's idea - join the table on itself - inner joins 
```{r}
full_join(all_manu, all_manu, by=c("related.manu"="manuscript.number")) %>% 
  select(manuscript.number, starts_with("related.manu")) %>% distinct() %>% unite(sep = ".")
```

Nick - subset on NA b/c those will all be first submissions, then join based on manu number - general idea is to shorten the grouping number by making it the "primary" submission. (group_by then sum ?)

**- Department seminar**

Set up meeting with him to discuss challenges in publishing

**- Bacillus manuscript**

Finished formatting. Just waiting for Zack's supplemental figure. 

**- Flux**

Permission denied.

**- ASM Fungi Blog Post**

Work on first draft. 

#2018/05/02

**- Bacillus manuscript**

Edited text, wrote importance section, figured out word counts and formatted for submission. 

**-Flux**

Got logged in to flux using the Windows PowerShell ssh client: `ssh akhagan@flux-login.arc-ts.umich.edu` & my level-1 password. Pat has stored all of the XML files in `/nfs/turbo/schloss-lab/gender_data` but I don't have access: `Permission denied`. For future reference there is a [Flux user guide](http://arc-ts.umich.edu/flux-user-guide/), a [helpful Youtube video](https://www.youtube.com/watch?v=D2qmlbCNqi4) & the [handouts](http://cscar.research.umich.edu/event/introduction-to-the-flux-cluster-and-batch-computing-5-3/) from a CSCAR class on intro to Flux. Also remember the notes from the commandline class!

**-XML parsing**

I'm only getting the manu.number back as the revised.manu.number. I think that's because it's been assigned in the beginning & isn't being re-written or the re-writing is being lost b/c of assigning the input_manu to it. So... how do I re-write it such that I don't lose the assignment? Flip it to a while statement? 

I tried bringing `revised.manu.number <- manu_number` below the `if` statement but that didn't work either. 

#2018/05/01

**- Prepping for Lab Meeting**

Wrote up an Rmarkdown with an explanation and all of the code for Code Review at lab meeting. 

**- Science Editor Seminar**

**- E.M. Blackwell**

Picked up & skimmed through "A Century of Mycology" for references. Didn't find much despite Webster mentioning other female BMS presidents who were _wives_ of other presidents/founding members?

**- XML parsing**

Saved `all_manu` as a csv file and started fresh script to import then work on building function to link manuscripts. I think I have something close. I have a `repeat` wrapping an `if else` with lots of pasting. 

**- Finish editing ASM blog post**